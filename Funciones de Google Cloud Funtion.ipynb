{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCIONES DE GOOGLE CLOUD FUNTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCION DEL ETL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from google.cloud import storage\n",
    "\n",
    "def process_csv(event, context):\n",
    "    try:\n",
    "        # Obtener detalles del archivo y los buckets\n",
    "        raw_bucket_name = \"carga_de_datos\"  # Nombre del bucket de archivos crudos\n",
    "        input_file = event['name']\n",
    "        print(f\"Archivo recibido para procesar: {input_file}\")\n",
    "        \n",
    "        # Bucket donde se guardarán los archivos limpios\n",
    "        clean_bucket_name = \"datos_limpioss\"  # Nombre del bucket de archivos limpios\n",
    "        print(f\"Bucket de origen: {raw_bucket_name}, Bucket de destino: {clean_bucket_name}\")\n",
    "        \n",
    "        # Ignorar archivos procesados por seguridad\n",
    "        if input_file.startswith(\"processed_\"):\n",
    "            print(f\"Ignorando archivo ya procesado: {input_file}\")\n",
    "            return\n",
    "        \n",
    "        # Definir el nombre del archivo de salida procesado\n",
    "        output_file = f\"processed_{input_file}\"\n",
    "        print(f\"Nombre del archivo de salida: {output_file}\")\n",
    "\n",
    "        # Inicializar cliente de Google Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "        print(\"Cliente de Google Cloud Storage inicializado\")\n",
    "\n",
    "        # Bucket crudo (origen)\n",
    "        raw_bucket = storage_client.bucket(raw_bucket_name)\n",
    "        \n",
    "        # Bucket limpio (destino)\n",
    "        clean_bucket = storage_client.bucket(clean_bucket_name)\n",
    "        print(\"Buckets obtenidos correctamente\")\n",
    "\n",
    "        # Descargar el archivo CSV desde el bucket crudo\n",
    "        blob = raw_bucket.blob(input_file)\n",
    "        data = blob.download_as_bytes()\n",
    "        print(f\"Archivo {input_file} descargado desde el bucket crudo\")\n",
    "        \n",
    "        # Comprobar si el archivo se descargó correctamente\n",
    "        if not data:\n",
    "            raise ValueError(f\"El archivo {input_file} está vacío o no se pudo descargar correctamente\")\n",
    "        \n",
    "        # Leer el contenido del archivo en un DataFrame\n",
    "        df = pd.read_csv(io.BytesIO(data))\n",
    "        print(f\"Archivo CSV cargado en un DataFrame. Columnas: {df.columns.tolist()}\")\n",
    "\n",
    "        # Procesar el DataFrame\n",
    "        df = procesar_dataframe(df)\n",
    "        print(f\"DataFrame procesado exitosamente\")\n",
    "\n",
    "        # Convertir el DataFrame a CSV y guardarlo en el bucket limpio\n",
    "        output_blob = clean_bucket.blob(output_file)\n",
    "        output_buffer = io.StringIO()\n",
    "        df.to_csv(output_buffer, index=False)\n",
    "        output_blob.upload_from_string(output_buffer.getvalue(), content_type='text/csv')\n",
    "        print(f\"Archivo procesado guardado en el bucket limpio como {output_file}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f'Error procesando el archivo {input_file}: {str(e)}')\n",
    "\n",
    "\n",
    "def procesar_dataframe(df):\n",
    "    try:\n",
    "        print(\"Iniciando procesamiento del DataFrame\")\n",
    "        \n",
    "        # Tratar valores nulos: 'sin dato' para strings/objects, 0 para enteros y 0.0 para flotantes\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'int64':\n",
    "                df[col].fillna(0, inplace=True)\n",
    "                print(f\"Valores nulos reemplazados por 0 en la columna {col}\")\n",
    "            elif df[col].dtype == 'float64':\n",
    "                df[col].fillna(0.0, inplace=True)\n",
    "                print(f\"Valores nulos reemplazados por 0.0 en la columna {col}\")\n",
    "            elif df[col].dtype == 'object':\n",
    "                df[col].fillna(\"sin dato\", inplace=True)\n",
    "                print(f\"Valores nulos reemplazados por 'sin dato' en la columna {col}\")\n",
    "\n",
    "        # Limpiar espacios, caracteres especiales y normalizar texto en columnas de texto\n",
    "        for col in df.select_dtypes(include=['object']).columns:\n",
    "            df[col] = df[col].str.strip()\n",
    "            df[col] = df[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "            df[col] = df[col].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "            df[col] = df[col].str.title()\n",
    "            df[col].replace('', 'Sin Dato', inplace=True)\n",
    "            print(f\"Columna {col} limpiada y normalizada\")\n",
    "\n",
    "        # Convertir valores no numéricos en columnas float a 0.0\n",
    "        for col in df.select_dtypes(include=['float64']).columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)\n",
    "            print(f\"Valores no numéricos convertidos a 0.0 en la columna {col}\")\n",
    "\n",
    "        # Eliminar duplicados basados en todas las columnas\n",
    "        df = df.drop_duplicates(keep='first')\n",
    "        print(\"Duplicados eliminados del DataFrame\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar el DataFrame: {str(e)}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCION QUE CARGA DATOS A BIGQUERY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# Inicializar clientes de Cloud Storage y BigQuery\n",
    "storage_client = storage.Client()\n",
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "def process_file_to_bigquery(event, context):\n",
    "    \"\"\"Esta función se activa cuando un archivo es subido al bucket de Cloud Storage.\"\"\"\n",
    "\n",
    "    # Obtener el nombre del bucket y del archivo\n",
    "    bucket_name = event['bucket']\n",
    "    file_name = event['name']\n",
    "\n",
    "    print(f\"Archivo {file_name} detectado en el bucket {bucket_name}.\")\n",
    "\n",
    "    # Definir el mapeo entre el nombre del archivo y las tablas de BigQuery\n",
    "    project_id = \"canvas-sum-435121-c0\"  # Asegúrate de usar el ID de proyecto correcto\n",
    "    dataset_id = \"Dataset_California\"  # Nombre correcto del dataset\n",
    "\n",
    "    table_mapping = {\n",
    "        'Business Yelp': f'{project_id}.{dataset_id}.Business_Yelp',\n",
    "        'Metadata Google': f'{project_id}.{dataset_id}.Metadata_Google',\n",
    "        'Reviews Google': f'{project_id}.{dataset_id}.Reviews_Google',\n",
    "        'Reviews Yelp': f'{project_id}.{dataset_id}.Reviews_Yelp',\n",
    "        'Yelp Reviews California': f'{project_id}.{dataset_id}.Yelp_Reviews_California'  # Nueva tabla añadida\n",
    "    }\n",
    "\n",
    "    # Verificar si el nombre del archivo coincide con alguna de las tablas\n",
    "    try:\n",
    "        for key in table_mapping:\n",
    "            if key in file_name:\n",
    "                table_id = table_mapping[key]\n",
    "                print(f\"El archivo {file_name} será cargado en la tabla {table_id}.\")\n",
    "                \n",
    "                # Intentar cargar el archivo en BigQuery\n",
    "                load_file_to_bigquery(bucket_name, file_name, table_id)\n",
    "                \n",
    "                # Verificación de éxito\n",
    "                print(f\"El archivo {file_name} se cargó correctamente en la tabla {table_id}.\")\n",
    "                return\n",
    "        \n",
    "        # Si no se encuentra una tabla correspondiente\n",
    "        print(f\"No se encontró una tabla correspondiente para el archivo {file_name}.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Manejo de errores\n",
    "        print(f\"Error durante el procesamiento del archivo {file_name}: {e}\")\n",
    "\n",
    "def load_file_to_bigquery(bucket_name, file_name, table_id):\n",
    "    \"\"\"Función para cargar el archivo de Cloud Storage a BigQuery.\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Obtener referencia al bucket y archivo\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "        \n",
    "        # Descargar el archivo temporalmente en la instancia de Cloud Function\n",
    "        temp_file = f\"/tmp/{file_name}\"\n",
    "        blob.download_to_filename(temp_file)\n",
    "        print(f\"Archivo {file_name} descargado temporalmente a {temp_file}.\")\n",
    "\n",
    "        # Configurar el trabajo de carga de BigQuery\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format=bigquery.SourceFormat.CSV,\n",
    "            skip_leading_rows=1,  # Asumimos que el archivo tiene encabezados\n",
    "            autodetect=True,  # Detectar automáticamente el esquema\n",
    "        )\n",
    "\n",
    "        # Abrir el archivo CSV y cargarlo en BigQuery\n",
    "        with open(temp_file, \"rb\") as source_file:\n",
    "            job = bigquery_client.load_table_from_file(source_file, table_id, job_config=job_config)\n",
    "\n",
    "        job.result()  # Esperar a que el job termine\n",
    "\n",
    "        print(f\"Archivo {file_name} cargado exitosamente en la tabla {table_id} de BigQuery.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Manejo de errores en el proceso de carga a BigQuery\n",
    "        print(f\"Error al cargar el archivo {file_name} a BigQuery: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
